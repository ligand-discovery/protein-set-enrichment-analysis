{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enrichment app data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1357294, 9)\n",
      "(1311295, 9)\n",
      "(45988, 9)\n"
     ]
    }
   ],
   "source": [
    "rename = {\n",
    "    \"Accession\": \"UniProtID\",\n",
    "    \"FragID\": \"FragID\",\n",
    "    \"Abundance_Ratio_Adj_PValue\": \"pAdj\",\n",
    "    \"Abundance_Ratio_log2\": \"Log2FC\",\n",
    "    \"Abundance_Ratio_log2_median_corr\": \"Log2FCMedian\",\n",
    "    \"Abundance_Ratio_PValue\": \"p\",\n",
    "    \"Rank_relative\": \"RankRelative\",\n",
    "    \"Number_of_Protein_Unique_Peptides\": \"UniqPeptides\",\n",
    "    \"exp_id\": \"expID\"\t\n",
    "}\n",
    "\n",
    "df = pd.read_csv(\"../data/general/screening.tsv\", sep=\"\\t\")\n",
    "df = df[[k for k in rename.keys()]]\n",
    "df = df.rename(columns = rename)\n",
    "df.to_csv(\"../data/general/cemm_primary_data.tsv\", sep=\"\\t\", index=False)\n",
    "print(df.shape)\n",
    "df = df[df[\"UniqPeptides\"] >= 2]\n",
    "df = df[df[\"p\"].notnull()]\n",
    "df.to_csv(\"../data/general/cemm_primary_detected_data.tsv\", sep=\"\\t\", index=False)\n",
    "print(df.shape)\n",
    "\n",
    "df = pd.read_csv(\"../data/general/screening_hits.tsv\", sep=\"\\t\")\n",
    "df = df[[k for k in rename.keys()]]\n",
    "df = df.rename(columns = rename)\n",
    "df.to_csv(\"../data/general/cemm_primary_hit_data.tsv\", sep=\"\\t\", index=False)\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/general/cemm_primary_data.tsv\", sep=\"\\t\")\n",
    "prots = set(df[\"UniProtID\"])\n",
    "with open(\"../data/general/pid2name_primary_all.tsv\", \"r\") as f:\n",
    "    with open(\"../data/general/pid2name_primary.tsv\", \"w\") as g:\n",
    "        reader = csv.reader(f, delimiter=\"\\t\")\n",
    "        writer = csv.writer(g, delimiter=\"\\t\")\n",
    "        for r in reader:\n",
    "            if r[0] in prots:\n",
    "                writer.writerow([r[0], r[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get code to generate protein annotations files, please refer to [this notebook](https://github.com/ligand-discovery/exploratory-analyses-miscellanea/blob/main/notebooks/09_ProteinEnrichment.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(\"../data/general/cemm_primary_detected_data.tsv\", delimiter=\"\\t\")\n",
    "de = pd.read_csv(\"../data/general/cemm_primary_hit_data.tsv\", delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms import bipartite\n",
    "import networkx as nx\n",
    "\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from(sorted(set(de[\"FragID\"])), bipartite=0)\n",
    "B.add_nodes_from(sorted(set(de[\"UniProtID\"])), bipartite=1)\n",
    "\n",
    "pairs = []\n",
    "for r in de[[\"FragID\", \"UniProtID\"]].values:\n",
    "    pairs += [(r[0], r[1])]\n",
    "\n",
    "B.add_edges_from(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "fids_e = sorted(set(de[\"FragID\"]))\n",
    "frag_betweenness_centrality = bipartite.centrality.betweenness_centrality(B, nodes=fids_e)\n",
    "frag_closeness_centrality = bipartite.centrality.closeness_centrality(B, nodes=fids_e)\n",
    "frag_degree_centrality = bipartite.centrality.degree_centrality(B, nodes=fids_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pids_e = sorted(set(de[\"UniProtID\"]))\n",
    "prot_betweenness_centrality = bipartite.centrality.betweenness_centrality(B, nodes=pids_e)\n",
    "prot_closeness_centrality = bipartite.centrality.closeness_centrality(B, nodes=pids_e)\n",
    "prot_degree_centrality = bipartite.centrality.degree_centrality(B, nodes=pids_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "R = []\n",
    "for pid in pids_e:\n",
    "    R += [[prot_degree_centrality[pid], prot_betweenness_centrality[pid], prot_closeness_centrality[pid]]]\n",
    "\n",
    "trf = PowerTransformer()\n",
    "#trf = QuantileTransformer(output_distribution=\"normal\")\n",
    "X = trf.fit_transform(R)\n",
    "X = np.mean(X, axis=1)\n",
    "R = []\n",
    "for x in X:\n",
    "    R += [[x]]\n",
    "trf = QuantileTransformer(output_distribution=\"normal\")\n",
    "X = trf.fit_transform(R).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.argsort(-X)\n",
    "\n",
    "promiscuity = []\n",
    "for i in idxs:\n",
    "    promiscuity += [[pids_e[i], X[i]]]\n",
    "\n",
    "with open(\"../data/signatures/proteins/global/promiscuity/promiscuity_gauss.tsv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for r in promiscuity:\n",
    "        writer.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in [50, 100, 250, 500]:\n",
    "    R = promiscuity[:t]\n",
    "    with open(\"../data/signatures/proteins/global/promiscuity/promiscuity_top_{0}.tsv\".format(t), \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for r in R:\n",
    "            writer.writerow([r[0]])\n",
    "\n",
    "for t in [50, 100, 250, 500]:\n",
    "    R = promiscuity[-t:]\n",
    "    with open(\"../data/signatures/proteins/global/promiscuity/promiscuity_bottom_{0}.tsv\".format(t), \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for r in R:\n",
    "            writer.writerow([r[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "dd_counts = collections.defaultdict(int)\n",
    "dd_upeps = collections.defaultdict(int)\n",
    "for r in dd[[\"UniProtID\", \"UniqPeptides\"]].values:\n",
    "    dd_counts[r[0]] += 1\n",
    "    dd_upeps[r[0]] += r[1]\n",
    "\n",
    "R = []\n",
    "pids_d = []\n",
    "for k,v in dd_counts.items():\n",
    "    R += [[v, dd_upeps[k]]]\n",
    "    pids_d += [k]\n",
    "\n",
    "trf = QuantileTransformer(output_distribution=\"normal\")\n",
    "X = trf.fit_transform(R)\n",
    "X = np.mean(X, axis=1)\n",
    "R = []\n",
    "for x in X:\n",
    "    R += [[x]]\n",
    "trf = QuantileTransformer(output_distribution=\"normal\")\n",
    "X = trf.fit_transform(R).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.argsort(-X)\n",
    "\n",
    "detectability = []\n",
    "for i in idxs:\n",
    "    detectability += [[pids_d[i], X[i]]]\n",
    "\n",
    "with open(\"../data/signatures/proteins/global/detectability/detectability_gauss.tsv\", \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for r in detectability:\n",
    "        writer.writerow(r)\n",
    "\n",
    "for t in [50, 100, 250, 500]:\n",
    "    R = detectability[:t]\n",
    "    with open(\"../data/signatures/proteins/global/detectability/detectability_top_{0}.tsv\".format(t), \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for r in R:\n",
    "            writer.writerow([r[0]])\n",
    "\n",
    "for t in [50, 100, 250, 500]:\n",
    "    R = detectability[-t:]\n",
    "    with open(\"../data/signatures/proteins/global/detectability/detectability_bottom_{0}.tsv\".format(t), \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for r in R:\n",
    "            writer.writerow([r[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "det = {}\n",
    "with open(\"../data/signatures/proteins/global/detectability/detectability_gauss.tsv\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for r in reader:\n",
    "        det[r[0]] = float(r[1])\n",
    "\n",
    "prom = {}\n",
    "with open(\"../data/signatures/proteins/global/promiscuity/promiscuity_gauss.tsv\", \"r\") as f:\n",
    "    reader = csv.reader(f, delimiter=\"\\t\")\n",
    "    for r in reader:\n",
    "        prom[r[0]] = float(r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "frag2values = collections.defaultdict(list)\n",
    "for r in dd[[\"FragID\", \"UniProtID\", \"Log2FC\"]].values:\n",
    "    frag2values[r[0]] += [(r[1], r[2])]\n",
    "\n",
    "frag2values = dict((k, sorted(v, key=lambda x: -x[1])) for k, v in frag2values.items())\n",
    "\n",
    "frag2hits = collections.defaultdict(set)\n",
    "for r in de[[\"FragID\", \"UniProtID\"]].values:\n",
    "    frag2hits[r[0]].update([r[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412/412 [00:00<00:00, 1536.62it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "prot2ranks = collections.defaultdict(list)\n",
    "for k, v in tqdm(frag2values.items()):\n",
    "    for i, x in enumerate(v):\n",
    "        prot2ranks[x[0]] += [i]\n",
    "\n",
    "prot2ranks_mean = dict((k, np.mean(v)) for k, v in prot2ranks.items())\n",
    "prot2ranks_std = dict((k, np.std(v)) for k, v in prot2ranks.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "for k, v in frag2values.items():\n",
    "    dirname = os.path.join(\"../data/signatures/proteins/fragment/\", k)\n",
    "    if os.path.exists(dirname):\n",
    "        shutil.rmtree(dirname)\n",
    "    os.mkdir(dirname)\n",
    "    with open(os.path.join(dirname, \"{0}_val_log2fc.tsv\".format(k)), \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for x in v:\n",
    "            writer.writerow(x)\n",
    "    #with open(os.path.join(dirname, \"{0}_val_zrank.tsv\".format(k)), \"w\") as f:\n",
    "    #    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    #    for i, x in enumerate(v):\n",
    "    #        z = (prot2ranks_mean[x[0]] - i) / prot2ranks_std[x[0]]\n",
    "    #        writer.writerow([x[0], z])\n",
    "    for top in [50, 100, 250, 500]:\n",
    "        v_ = v[:top]\n",
    "        with open(os.path.join(dirname, \"{0}_bin_{1}.tsv\".format(k, top)), \"w\") as f:\n",
    "            writer = csv.writer(f, delimiter=\"\\t\")\n",
    "            for x in v_:\n",
    "                writer.writerow([x[0]])\n",
    "    hits = frag2hits[k]\n",
    "    hits = [x[0] for x in v if x[0] in hits]\n",
    "    with open(os.path.join(dirname, \"{0}_bin_hit.tsv\".format(k)), \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for x in hits:\n",
    "            writer.writerow([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global properties of proteins\n",
    "\n",
    "# Detectability\n",
    "dirname = os.path.join(\"../data/signatures/proteins/global/detectability\")\n",
    "if os.path.exists(dirname):\n",
    "    shutil.rmtree(dirname)\n",
    "os.mkdir(dirname)\n",
    "d = collections.defaultdict(int)\n",
    "for p in list(dd[\"UniProtID\"]):\n",
    "    d[p] += 1\n",
    "v = sorted(d.items(), key=lambda x: -x[1])\n",
    "with open(os.path.join(dirname, \"detectability_val_counts.tsv\"), \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for x in v:\n",
    "        writer.writerow(x)\n",
    "for top in [1500, 3000]:\n",
    "    with open(os.path.join(dirname, \"detectability_bin_{0}.tsv\".format(top)), \"w\") as f:\n",
    "        v_ = v[:top]\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for x in v_:\n",
    "            writer.writerow([x[0]])\n",
    "\n",
    "# Number of hits\n",
    "dirname = os.path.join(\"../data/signatures/proteins/global/promiscuity_hit\")\n",
    "if os.path.exists(dirname):\n",
    "    shutil.rmtree(dirname)\n",
    "os.mkdir(dirname)\n",
    "d = collections.defaultdict(int)\n",
    "for p in list(de[\"UniProtID\"]):\n",
    "    d[p] += 1\n",
    "v = sorted(d.items(), key=lambda x: -x[1])\n",
    "with open(os.path.join(dirname, \"promiscuity_hit_val_counts.tsv\"), \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for x in v:\n",
    "        writer.writerow(x)\n",
    "for top in [50, 100, 250, 500]:\n",
    "    with open(\n",
    "        os.path.join(dirname, \"promiscuity_hit_bin_{0}.tsv\".format(top)), \"w\"\n",
    "    ) as f:\n",
    "        v_ = v[:top]\n",
    "        writer = csv.writer(f, delimiter=\"\\t\")\n",
    "        for x in v_:\n",
    "            writer.writerow([x[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_specificity\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "dirname = os.path.join(\"../data/signatures/proteins/global/tfidf_specificity\")\n",
    "if os.path.exists(dirname):\n",
    "    shutil.rmtree(dirname)\n",
    "os.mkdir(dirname)\n",
    "pairs = []\n",
    "for v in de[[\"UniProtID\", \"FragID\"]].values:\n",
    "    pairs += [(v[0], v[1])]\n",
    "pairs = set(pairs)\n",
    "\n",
    "pids = sorted(set([p[0] for p in pairs]))\n",
    "fids = sorted(set([p[1] for p in pairs]))\n",
    "X = np.zeros((len(pids), len(fids)))\n",
    "for i, pid in enumerate(pids):\n",
    "    for j, fid in enumerate(fids):\n",
    "        if (pid, fid) in pairs:\n",
    "            X[i, j] += 1\n",
    "\n",
    "transf = TfidfTransformer()\n",
    "Tp = np.array(transf.fit_transform(X).todense())\n",
    "spec = 1 / np.sum(Tp, axis=1)\n",
    "\n",
    "d = {}\n",
    "for p, s in zip(pids, spec):\n",
    "    d[p] = s\n",
    "d = sorted(d.items(), key=lambda x: -x[1])\n",
    "\n",
    "with open(os.path.join(dirname, \"tfidf_specificity_val_by_sum.tsv\"), \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for r in d:\n",
    "        writer.writerow(r)\n",
    "\n",
    "transf = TfidfTransformer()\n",
    "Tf = np.array(transf.fit_transform(X.T).todense())\n",
    "fspec = 1 / np.sum(Tf, axis=1)\n",
    "\n",
    "d = {}\n",
    "for i, pid in enumerate(pids):\n",
    "    mask = X[i, :] > 0\n",
    "    d[pid] = np.mean(fspec[mask])\n",
    "d = sorted(d.items(), key=lambda x: -x[1])\n",
    "\n",
    "with open(os.path.join(dirname, \"tfidf_specificity_val_by_mean_frag.tsv\"), \"w\") as f:\n",
    "    writer = csv.writer(f, delimiter=\"\\t\")\n",
    "    for r in d:\n",
    "        writer.writerow(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
